# LORA EXPERIMENTS
I want to see which on the following technqiues performs the best across the following scales:
1. Full finetuning 
2. Lora
3. Dora
across 8,16,32 rank adapters. Maybe pushing to larger adapters if higher rank is winning.