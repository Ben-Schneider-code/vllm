{
  "model_name_or_path": "/home/b3schnei/pretrained/InternVL2-1B",
  "conv_style": "Hermes-2",
  "output_dir": "/home/b3schnei/output",
  "meta_path": "./shell/data/internvl_1_2_finetune_custom.json",
  "overwrite_output_dir": true,
  "force_image_size": 448,
  "max_dynamic_patch": 6,
  "down_sample_ratio": 0.5,
  "drop_path_rate": 0.1,
  "freeze_llm": true,
  "freeze_mlp": true,
  "freeze_backbone": true,
  "use_backbone_lora": 16,
  "use_llm_lora": 16,
  "vision_select_layer": -1,
  "dataloader_num_workers": 4,
  "bf16": true,
  "num_train_epochs": 1,
  "per_device_train_batch_size": 128,
  "gradient_accumulation_steps": 1,
  "evaluation_strategy": "no",
  "save_strategy": "steps",
  "save_steps": 500,
  "save_total_limit": 100,
  "learning_rate": 4e-5,
  "weight_decay": 0.01,
  "warmup_ratio": 0.03,
  "lr_scheduler_type": "cosine",
  "logging_steps": 1,
  "max_seq_length": 4096,
  "do_train": true,
  "grad_checkpoint": true,
  "group_by_length": false,
  "dynamic_image_size": true,
  "remove_unused_columns": false,
  "use_thumbnail": true,
  "ps_version": "v2",
  "deepspeed": "./deepspeed/zero_lora.json",
  "report_to": "wandb",
  "attn_mask": "causal",
  "loss_type": "last_token",
  "gather_loss": true
}