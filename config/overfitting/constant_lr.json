{
  "model_name_or_path": "/home/b3schnei/pretrained/InternVL2-1B",
  "conv_style": "Hermes-2",
  "output_dir": "/home/b3schnei/output/lora_last_token_bidirectional_local",
  "meta_path": "./shell/data/internvl_1_2_finetune_custom.json",
  "overwrite_output_dir": true,
  "force_image_size": 448,
  "max_dynamic_patch": 6,
  "down_sample_ratio": 0.5,
  "drop_path_rate": 0.1,
  "freeze_llm": true,
  "freeze_mlp": true,
  "freeze_backbone": true,
  "use_backbone_lora": 16,
  "use_llm_lora": 16,
  "vision_select_layer": -1,
  "dataloader_num_workers": 4,
  "bf16": true,
  "num_train_epochs": 1,
  "per_device_train_batch_size": 128,
  "gradient_accumulation_steps": 1,
  "save_strategy": "steps",
  "save_steps": 2000,
  "save_total_limit": 100,
  "learning_rate": 4e-5,
  "weight_decay": 0.01,
  "warmup_ratio": 0,
  "lr_scheduler_type": "constant",
  "logging_steps": 1,
  "max_seq_length": 4096,
  "do_train": true,
  "grad_checkpoint": true,
  "group_by_length": false,
  "dynamic_image_size": true,
  "remove_unused_columns": false,
  "use_thumbnail": true,
  "ps_version": "v2",
  "deepspeed": "./deepspeed/zero_lora.json",
  "report_to": "wandb",
  "attn_mask": "bidirectional",
  "eval_datasets": [
    "cc",
    "mscoco"
  ],
  "eval_steps_per_dataset": 20,
  "evaluation_strategy": "steps",
  "per_device_eval_batch_size": 128,
  "eval_steps": 50,
  "training_dataset_name": "cc128k"
}