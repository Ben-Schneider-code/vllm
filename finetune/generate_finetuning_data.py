import os
import orjson
from PIL import Image
from vllm import LLM, SamplingParams

# Set up CUDA devices and data path
os.environ["CUDA_VISIBLE_DEVICES"] = "1,2"
data_path = os.environ["WIKIWEB"]

# Load the dataset
with open(data_path, "rb") as file:
    data = orjson.loads(file.read())

# ---------------------------
# Configuration
# ---------------------------
MODEL_NAME = "Qwen/Qwen-VL-72B"  # Replace with the correct model name
MAX_TOKENS = 512
TEMPERATURE = 0.0  # For deterministic output, set temperature to 0.
TOP_P = 1.0
BATCH_SIZE = 1

# Initialize the LLM
engine = LLM(
    model=MODEL_NAME,
    tensor_parallel_size=2,     # Increase if running on multiple GPUs
    dtype="float16",            # Try "float16" or "bfloat16" on GPUs that support it
    load_in_8bit=False,         # Consider 8-bit if supported and needed for memory efficiency
    trust_remote_code=True      # If the model requires custom code from HF
)

def preprocess_image(image_path):
    """
    Preprocess the image for the model. You may need to adapt this 
    depending on the model's requirements.
    """
    image = Image.open(image_path).convert("RGB")
    # Add specific preprocessing steps here if required (e.g., resizing, normalization)
    return image

def process_batch(images, texts):
    """
    Pass a batch of images and corresponding text to the LLM for inference.

    Args:
        images (list): List of image file paths.
        texts (list): List of text strings corresponding to the images.

    Returns:
        list: The outputs generated by the LLM.
    """
    if len(images) != len(texts):
        raise ValueError("The number of images must match the number of texts.")

    # Preprocess images
    preprocessed_images = [preprocess_image(image) for image in images]

    # Sampling parameters
    sampling_params = SamplingParams(
        temperature=TEMPERATURE,
        top_p=TOP_P,
        max_tokens=MAX_TOKENS
    )

    # Prepare inputs for the LLM
    batch_inputs = []
    for img, txt in zip(preprocessed_images, texts):
        batch_inputs.append({
            "image": img,  # Replace with your model's required image input format
            "text": txt
        })

    # Generate outputs
    outputs = engine.generate(batch_inputs, sampling_params)
    return outputs

# Example Usage
example_images = ["path_to_image1.jpg", "path_to_image2.jpg"]
example_texts = ["Describe this image", "What do you see in this image?"]

outputs = process_batch(example_images, example_texts)
for output in outputs:
    print(output)




